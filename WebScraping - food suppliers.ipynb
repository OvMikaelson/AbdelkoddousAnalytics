{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "#first let's pick one company's info page at random and scrape from it the headers for our database\n",
    "url = \"https://www.organic-bio.com/en/company/12054-ALVARADO-STREET-BAKERY\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "page = requests.get(url,headers=headers)\n",
    "code = BeautifulSoup(page.content, \"html.parser\")\n",
    "col_names = code.find_all('th')\n",
    "col_names = [x.text for x in col_names]\n",
    "col_names.insert(0,'category')\n",
    "import csv\n",
    "with open(\"database.csv\", 'w', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(col_names)\n",
    "\n",
    "#now for every category let's get the basic url from which we'll get lists of all the pages of data it has\n",
    "url = \"https://www.organic-bio.com/en/directory/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "page = requests.get(url,headers=headers)\n",
    "code = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "categories = code.find_all('a')\n",
    "categories = [x.text.strip() for x in categories]\n",
    "categories = categories[47:167:3]\n",
    "\n",
    "links = code.find_all('a', href=True)\n",
    "links = [x['href'] for x in links]\n",
    "links = links[47:165:3]\n",
    "first_pages = []\n",
    "for x in links:\n",
    "    first_pages.append(url+x)\n",
    "\n",
    "#at this point we have the link to the first page in each category\n",
    "for del1 in range(len(categories)):\n",
    "    category = categories[del1]\n",
    "    url2= first_pages[del1]+'?groupService=46'\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    page2 = requests.get(url2,headers=headers)\n",
    "    code2 = BeautifulSoup(page2.content, \"html.parser\")\n",
    "\n",
    "    data = code2.find_all('a', href=True)\n",
    "    data = [x['href'] for x in data]\n",
    "\n",
    "    data2 = data[len(data)-3]\n",
    "    if '&' in data2:\n",
    "        page_last = int(data2[6:data2.index('&')])\n",
    "    else:\n",
    "        page_last = 1\n",
    "\n",
    "    final_links = []\n",
    "    for i in range(page_last):\n",
    "        final_links.append('?page='+str(i+1)+'&groupService=46')\n",
    "\n",
    "    final_links2 = []\n",
    "    for x in final_links:\n",
    "        final_links2.append(first_pages[del1]+x)\n",
    "\n",
    "#now we go through each of them getting a list of urls for all the pages each category has\n",
    "    for del2 in range(len(final_links2)):\n",
    "        url3= final_links2[del2]\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "        page3 = requests.get(url3,headers=headers)\n",
    "        code3 = BeautifulSoup(page3.content, \"html.parser\")\n",
    "\n",
    "        companies = code3.find_all('a', href=True)\n",
    "        companies = [x['href'] for x in companies]\n",
    "\n",
    "        companies2 = []\n",
    "        for i in range(len(companies)):\n",
    "            m = companies[i].startswith('https://www.organic-bio.com/en/company/')\n",
    "            if m == True:\n",
    "                companies2.append(companies[i])\n",
    "\n",
    "#finally all we have to do is scrape the companies' data one by one from each url and add it as a new row to our csv\n",
    "        for del3 in range(len(companies2)):\n",
    "            url4 = companies2[del3]\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "            page4 = requests.get(url4,headers=headers)\n",
    "            code4 = BeautifulSoup(page4.content, \"html.parser\")\n",
    "\n",
    "            details = code4.find_all('td')\n",
    "            details2 = [x.text.strip() for x in details]\n",
    "            details2 = details2[3:len(details2)-1]\n",
    "            details2.insert(0,category)\n",
    "\n",
    "            with open(\"database.csv\", 'a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(details2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
